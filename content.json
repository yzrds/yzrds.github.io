{"meta":{"title":"梁友泽的博客","subtitle":null,"description":null,"author":"梁友泽","url":"https://www.liangyouze.com"},"pages":[{"title":"","date":"2021-02-01T01:52:37.268Z","updated":"2020-01-05T14:32:42.522Z","comments":true,"path":"about/index.html","permalink":"https://www.liangyouze.com/about/index.html","excerpt":"","text":"梁友泽个人信息 籍贯 重庆 Github https://github.com/youzeliang 技术博客 https://www.liangyouze.com email: youzedev@gmail.com 就职于某人工智能公司，负责方向:仓储业务"},{"title":"Categories","date":"2019-12-16T02:23:19.516Z","updated":"2018-11-18T12:34:28.000Z","comments":true,"path":"categories/index.html","permalink":"https://www.liangyouze.com/categories/index.html","excerpt":"","text":""},{"title":"categories","date":"2018-08-28T13:38:07.000Z","updated":"2018-08-28T13:38:07.000Z","comments":true,"path":"categories/index-LiangYouze-1.html","permalink":"https://www.liangyouze.com/categories/index-LiangYouze-1.html","excerpt":"","text":""},{"title":"","date":"2019-12-16T02:23:19.515Z","updated":"2018-08-27T15:15:37.000Z","comments":true,"path":"archives/index.html","permalink":"https://www.liangyouze.com/archives/index.html","excerpt":"","text":"ceshi"}],"posts":[{"title":"给字符串加索引","slug":"给字符串加索引","date":"2020-02-11T06:10:23.000Z","updated":"2020-03-18T00:28:51.579Z","comments":true,"path":"2020/02/11/给字符串加索引/","link":"","permalink":"https://www.liangyouze.com/2020/02/11/给字符串加索引/","excerpt":"MySQL是支持前缀索引的，前缀索引的优势就是占用的空间小，这同时带来的损失是，可能会增加额外的记录扫描次数。","text":"MySQL是支持前缀索引的，前缀索引的优势就是占用的空间小，这同时带来的损失是，可能会增加额外的记录扫描次数。 比如一些用户表，登录账户是邮箱 如果要使用的是邮箱登录，所以代码中一定会有这种类似的语句 1select f1, f2 from tableName where email='xxx'; 如果email这个字段上没有索引的话，那这些语句就只能做全表扫描 MySQL 是支持前缀索引的，可以定义字符串的一部分作为索引。默认地，如果你创建索引的语句不指定前缀长度，那么索引就会包含整个字符串。 比如，这两个在 email 字段上创建索引的语句： 12alter table t add index index1(email);alter table t add index index2(email(6)); 第一个语句创建的 index1 索引里面，包含了每个记录的整个字符串； 而第二个语句创建的 index2 索引里面，对于每个记录都是只取前 6 个字节。 其中email(6)这个索引结构中每个邮箱字段只取前6个字节，占用的空间会比较小，这是使用前缀索引的优势,但是带来的损失可能会增加额外的记录扫描次数 看看下面这个语句 1select id,name,email from SUser where email='zhangssxyz@xxx.com'; 如果使用的是这种索引index1（即 email 整个字符串的索引结构），执行顺序是这样的： 从 index1 索引树找到满足索引值是’zhangssxyz@xxx.com’的这条记录，取得 ID2 的值； 到主键上查到主键值是 ID2 的行，判断 email 的值是正确的，将这行记录加入结果集； 取 index1 索引树上刚刚查到的位置的下一条记录，发现已经不满足 email=&#39;zhangssxyz@xxx.com’的条件了，循环结束。 这个过程中，只需要回主键索引取一次数据，所以系统认为只扫描了一行。 如果使用的是 index2（即 email(6) 索引结构），执行顺序是这样的： 从 index2 索引树找到满足索引值是’zhangs’的记录，找到的第一个是 ID1； 到主键上查到主键值是 ID1 的行，判断出 email 的值不是’zhangssxyz@xxx.com’，这行记录丢弃； 取 index2 上刚刚查到的位置的下一条记录，发现仍然是’zhangs’，取出 ID2，再到 ID 索引上取整行然后判断，这次值对了，将这行记录加入结果集； 重复上一步，直到在 idxe2 上取到的值不是’zhangs’时，循环结束。 在这个过程中，要回主键索引取 4 次数据，也就是扫描了 4 行。 所以使用前缀索引有可能会使查询语句读数据的次数变多 使用前缀索引，定义好长度，就可以做到既节省空间，又不用额外增加太多的查询成本。 如果我们能够确定业务需求里面只有按照身份证进行等值查询的需求，这种方法，既可以占用更小的空间，也能达到相同的查询效率。 有以下2中方式 就是使用倒序存储，比如身份证倒序，查询的时候再用函数转一下 以及使用hash字段，在表上创建一个整数字段，来保存身份证的校验码，同时在这个字段上加索引 这两种方式对比区别 从占用的额外空间来看，倒序存储方式在主键索引上，不会消耗额外的存储空间，而 hash 字段方法需要增加一个字段。当然，倒序存储方式使用 4 个字节的前缀长度应该是不够的，如果再长一点，这个消耗跟额外这个 hash 字段也差不多抵消了。 在 CPU 消耗方面，倒序方式每次写和读的时候，都需要额外调用一次 reverse 函数，而 hash 字段的方式需要额外调用一次 crc32() 函数。如果只从这两个函数的计算复杂度来看的话，reverse 函数额外消耗的 CPU 资源会更小些。 从查询效率上看，使用 hash 字段方式的查询性能相对更稳定一些。因为 crc32 算出来的值虽然有冲突的概率，但是概率非常小，可以认为每次查询的平均扫描行数接近 1。而倒序存储方式毕竟还是用的前缀索引的方式，也就是说还是会增加扫描行数。","categories":[{"name":"mysql","slug":"mysql","permalink":"https://www.liangyouze.com/categories/mysql/"}],"tags":[{"name":"索引","slug":"索引","permalink":"https://www.liangyouze.com/tags/索引/"}]},{"title":"2019年总结","slug":"2019年总结","date":"2020-01-01T09:10:23.000Z","updated":"2020-01-01T09:20:51.813Z","comments":true,"path":"2020/01/01/2019年总结/","link":"","permalink":"https://www.liangyouze.com/2020/01/01/2019年总结/","excerpt":"今年的最后一段时间是在罗振宇的跨年演讲中度过的，当然不是在现场。 2019是一个有意思的一年，如在北京待的越久，似乎就越不愿意离开了。在这里认识了更多厉害的人，就会看到自己的渺小。","text":"今年的最后一段时间是在罗振宇的跨年演讲中度过的，当然不是在现场。 2019是一个有意思的一年，如在北京待的越久，似乎就越不愿意离开了。在这里认识了更多厉害的人，就会看到自己的渺小。自己的认知也在逐渐走向远方，看到更远的视野。在这一年里，美国不断打压中国，我所在的公司也在美国的”名单”里，以及看到各大公司裁员，这似乎就有一种焦虑，被时代驱逐前行的我们，是容不得“舒适”的环境。但你的性格，眼界格局，认知等或许就已经决定了在这个格局变化里，下一个地方会何去何从。有庆幸，自己身处在这行业里，不说那些伟大的事情(普通人完全是扯淡)，但能凭借自己的能力，也能自立自足。但又有些不甘，不甘的是野心和努力不成正比吧。或许还是对自己不够狠。不甘中又有一丝惧怕，似乎看到了所谓的35岁后的自己。或许努力生活的人，都自在执着属性吧。 社会是多元化的，一朵朵海浪中荡漾着不同光芒的思想 感谢 从2017年末yqj就开始鼓励我，今年也是不例外。总是交流了一些超越我现在年龄阶段认知的一些思想，她自己也工作了4-5年后去港大上学了。优秀的人是真的没有停止下来。是最最感激的人之一了 然后就是qmj这个本该3月就该见一面认识的，话说都认识了好久了，一个很乐观，阳光的一个人，交流起来也挺有意思的。以积极向上，玩耍的心态工作(或许没有)生活，同样很感谢 zs这个每天都是打了鸡血的人，太拼了同时也太强了。说实话也影响了我很多，他的成长还是看在眼里的，就是有时候做事有点怪 还有其他一些人 然后回顾下回想一下2019的flag 早睡早起 上半年大概还能做到，然后到下半年的时候就逐渐又回到了之前的状态了，主要是真的早起动力不足哇。 阅读20本+ 书籍 回顾了下，2019年看完了34本书，其中数学(主要是科普类的了，因为再看一些类似于刷题之类的没啥意义了)书籍占了近40%吧，有历史(对，开始看自己以前不喜欢的历史了)，经济，当然还有一些技术类的书籍。总之来看，普遍是一些实用类的，反正是不会看文学类的书籍 参加不少于5场有价值的线下技术会议 大概4场吧，虽然有自己想深入去了解的方向，但是自制力是真的不够，时间眨眼就过去了。至少也算是开拓了眼界，认识到了一些厉害的人。这大概也是鞭挞前进的动力吧 看电影少于20部 好像不知不觉看了60部,这过分了 输出20篇有质量的博客 不到一半吧，还是需要继续沉淀 脱单 这真的是太难了,所以这是要继续作为2020年的目标么 投入一定时间在学习英语上 至少看一些技术文档优先于官网的，以及中途尝试翻译了一些英语技术文章，还有一篇得到了国外的一个技术人员的认可。大部分的时间通过英语文章来了解国际新闻，这当然国内的微博热搜也是一种途径 至少一次一个人的旅行 真没想到这也没完成，那就2020年换公司了来吧 一场音乐会 倒不是有这细胞，只是作为程序员感觉要多接触其他不同方面的东西，这让我想起了最近看的一个知乎问题，作为程序员你失去了什么，专注在自己喜欢的领域里是值得肯定的，但是圈子外面的世界也会是很精彩的 开始了解经济,股票 今年的收益率还是行了吧，投入的钱不多 做饭，少点外卖 这，上半年还在坚持做，但是到了10月份，就几乎没有了。一是隔壁租的其他人用厨房的频率高了，又不想你等我，我等你的这种 跑完60个5km，5个10km 大概在9月份吧，就完成了。本来以为今年有机会去参加半马的(不过这不在flag之内，看明年锻炼的情况)，但是看着锻炼的最长距离感觉还不行。而且主要目的又不是非要去参加半马。锻炼意志力了就行，身体锻炼是其次 2019过得并没有像自己想象中的样子，尽量在2020年重新拾起一些东西吧，勇敢大步向前走，去自己想去的公司。2020年的flag就不先公开了，希望在技术上像贝聿铭先生说的那样，”我一直沉浸在如何解决我自己的问题之中”,你成长的速度必须足够快，才能抓住一些你要抓住的东西以及未来想抓住的东西 未来可期","categories":[{"name":"杂谈","slug":"杂谈","permalink":"https://www.liangyouze.com/categories/杂谈/"}],"tags":[{"name":"杂谈","slug":"杂谈","permalink":"https://www.liangyouze.com/tags/杂谈/"}]},{"title":"数据删掉一半，表的大小不变","slug":"数据删掉一半，表的大小不变","date":"2019-12-17T06:10:23.000Z","updated":"2020-03-18T00:35:59.635Z","comments":true,"path":"2019/12/17/数据删掉一半，表的大小不变/","link":"","permalink":"https://www.liangyouze.com/2019/12/17/数据删掉一半，表的大小不变/","excerpt":"数据库占用空间太大，把一个最大的表删掉了一半的数据，怎么表文件的大小还是没变？","text":"数据库占用空间太大，把一个最大的表删掉了一半的数据，怎么表文件的大小还是没变？ 先来看看这块儿知识 一个 InnoDB 表包含两部分，即：表结构定义和数据。在 MySQL 8.0 版本以前，表结构是存在以.frm 为后缀的文件里。而 MySQL 8.0 版本，则已经允许把表结构定义放在系统数据表中了。因为表结构定义占用的空间很小 其中有一个参数innodb_file_per_table，它的值有on和off，OFF 表示的是，表的数据放在系统共享表空间，也就是跟数据字典放在一起；ON 表示的是，每个 InnoDB 表数据存储在一个以 .ibd 为后缀的文件中。从 MySQL 5.6.6 版本开始，它的默认值就是 ON 了。 比如要删除ID=5的对应的数据(称为R1)，InnoDB 引擎只会把R1这个记录标记为删除。如果之后要再插入一个ID在4和6之间的记录时，可能会复用这个位置。但是，磁盘文件的大小并不会缩小。InnoDB 的数据是按页存储的，如果删除一个数据页上所有的记录，整个数据页就可以被复用了。 数据页的复用跟记录的复用是不同的 记录的复用，只限于符合范围条件的数据。比如上面的这个例子，R1 这条记录被删除后，如果插入一个 ID 是 400 的行，可以直接复用这个空间。但如果插入的是一个 ID 是 800 的行，就不能复用这个位置了。 而当整个页从 B+ 树里面摘掉以后，可以复用到任何位置。如果将数据页 page A 上的所有记录删除以后，page A 会被标记为可复用。这时候如果要插入一条 ID=50 的记录需要使用新页的时候，page A 是可以被复用的。 如果相邻的两个数据页利用率都很小，系统就会把这两个页上的数据合到其中一个页上，另外一个数据页就被标记为可复用。 进一步地，如果我们用 delete 命令把整个表的数据删除呢？结果就是，所有的数据页都会被标记为可复用。但是磁盘上，文件不会变小。 delete 命令其实只是把记录的位置，或者数据页标记为了“可复用”，但磁盘文件的大小是不会变的。也就是说，通过 delete 命令是不能回收表空间的。这些可以复用，而没有被使用的空间，看起来就像是“空洞”。 不止是删除数据会造成空洞，插入数据也会。如果数据是按照索引递增顺序插入的，那么索引是紧凑的。但如果数据是随机插入的，就可能造成索引的数据页分裂。 比如在乱序的时候插入ID=500的时候，然后就会去申请新的页面page B来保存数据了， 经过大量增删改的表，都是可能是存在空洞的。所以，如果能够把这些空洞去掉，就能达到收缩表空间的目的。 而重建表，就可以达到这样的目的。 可以使用 alter table A engine=InnoDB 命令来重建表。在 MySQL 5.5 版本之前，这个命令的执行流程跟我们前面描述的差不多，区别只是这个临时表 B 不需要你自己创建，MySQL 会自动完成转存数据、交换表名、删除旧表的操作。(如果在这个过程中，有新的数据要写入到表 A 的话，就会造成数据丢失。因此，在整个 DDL 过程中，表 A 中不能有更新。也就是说，这个 DDL 不是 Online 的。) MySQL 5.6 版本开始引入的 Online DDL，对这个操作流程做了优化。 重建表的流程： 建立一个临时文件，扫描表 A 主键的所有数据页； 用数据页中表 A 的记录生成 B+ 树，存储到临时文件中； 生成临时文件的过程中，将所有对 A 的操作记录在一个日志文件（row log）中，对应的是图中 state2 的状态； 临时文件生成后，将日志文件中的操作应用到临时文件，得到一个逻辑数据上与表 A 相同的数据文件，对应的就是图中 state3 的状态； 用临时文件替换表 A 的数据文件。","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://www.liangyouze.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://www.liangyouze.com/tags/MySQL/"}]},{"title":"分析MySQL中隐式转换导致查询结果错误及索引不可用","slug":"分析MySQL中隐式转换导致查询结果错误及索引不可用","date":"2019-10-11T06:10:23.000Z","updated":"2019-10-13T02:20:49.007Z","comments":true,"path":"2019/10/11/分析MySQL中隐式转换导致查询结果错误及索引不可用/","link":"","permalink":"https://www.liangyouze.com/2019/10/11/分析MySQL中隐式转换导致查询结果错误及索引不可用/","excerpt":"以下是例子 1SELECT * FROM TABLE WHERE xxx = 11 如果列xxx确实只有11的，你是否就认为筛选出来的就一定只有xxx=11的呢？ 在过滤字段为数值类型的时候，数值类型有一种隐式转换，如果是以数字开头的，包含有字符，后面的字符会被截断，只取前面的数字值。","text":"以下是例子 1SELECT * FROM TABLE WHERE xxx = 11 如果列xxx确实只有11的，你是否就认为筛选出来的就一定只有xxx=11的呢？ 在过滤字段为数值类型的时候，数值类型有一种隐式转换，如果是以数字开头的，包含有字符，后面的字符会被截断，只取前面的数字值。 以下也均为测试数据 当执行1explain select * from business_flow where business_flow_id = 268805964457574426 看输出会出现这段话 Cannot use ref access on index ‘xxx’ due to type or collation conversion on field ‘business_flow_id’ 当过滤的字段是字符类型的时候，没有使用到索引，走的全表扫描； 所以还是可以查询出结果来的，因为无法使用索引，所以查询出来的结果也是错的。 既然发现查询出来的结果是有误差的，所以猜测用字符串’xxx’和xxy比较应该是相等的。 1select '268805964457574426' =268805964457574421 果不其然，也能查询出 去查询了下其他的 过滤字段为浮点类型，也会比较近似的，将导致结果看起来不一致，也就是可能导致查询结果错误 当MySQL遇到字段类型不匹配的时候，会进行各种隐式转化 所以在查询过滤的时候，一定要注意过滤字段的类型。可能会导致查询慢，甚至会导致错误结果。 官方说是隐式转换 参考","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://www.liangyouze.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://www.liangyouze.com/tags/MySQL/"}]},{"title":"记一次MySQL死锁排查过程","slug":"MySQL死锁","date":"2019-09-11T06:10:23.000Z","updated":"2019-10-13T08:36:36.268Z","comments":true,"path":"2019/09/11/MySQL死锁/","link":"","permalink":"https://www.liangyouze.com/2019/09/11/MySQL死锁/","excerpt":"背景大概说一下业务场景，需要定时计算一些数据，从其他系统、接口拉取达到的数据比较多，然后经计算后的值存储在本系统中。拉取的数据量可能有万左右，然后以主键存在的数据是需要更新的。不存在则插入。每次做全量更新/插入。","text":"背景大概说一下业务场景，需要定时计算一些数据，从其他系统、接口拉取达到的数据比较多，然后经计算后的值存储在本系统中。拉取的数据量可能有万左右，然后以主键存在的数据是需要更新的。不存在则插入。每次做全量更新/插入。 起因最开始采用的方法是先查询，数据存在则更新数据，不存在则插入数据。但是数据要求的时效性比较高。于是定时任务在做任务处理的时候频率就比较高了。就出现了单位时间内对数据库的读写高，于是就换了一个方法。用INSERT … ON DUPLICATE KEY UPDATE。 对数据库的读写次数虽然比之前少了，但是又引发了一个新的问题，因为更新、插入的数据量多，所以导致与一条INSERT … ON DUPLICATE KEY UPDATE的执行时间有点长，大概5s，去研究了下。实际上一次批量插入几千条条数据。为了解决这个问题，就分组批量查询，分为了每 50 条数据一组，这样每条sql 执行的时间也就短了。 随之又出现了另外一个问题，随着数据量的增加，一次循环拉取的数据经计算,写入,更新等。时间大于定时任务所处理的时间，这样就导致与上一个定时任务还没处理完的时候，下一个定时任务又进来处理数据了。 于是选择的做法是在每处理一组数据的时候，把 redis 的key 延长一点时间。然后整组数据处理完的时候，再删除 redis 的 值。等下一次定时任务抢到锁了再进来处理。 想到的第二个方案是，直接插入数据库，主键冲突就抛错，根据指定的错来更新值","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://www.liangyouze.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://www.liangyouze.com/tags/MySQL/"}]},{"title":"合适以及为何使用最少使用(LFU)缓存与Golang中的实现","slug":"[译]合适以及为何使用最少使用(LFU)缓存与Golang中的实现","date":"2019-06-04T12:10:33.000Z","updated":"2019-06-04T15:29:03.332Z","comments":true,"path":"2019/06/04/[译]合适以及为何使用最少使用(LFU)缓存与Golang中的实现/","link":"","permalink":"https://www.liangyouze.com/2019/06/04/[译]合适以及为何使用最少使用(LFU)缓存与Golang中的实现/","excerpt":"[译]合适以及为何使用最少使用(LFU)缓存与Golang中的实现在过去的这些年，参与计算机科学和工程师的人们一直在努力优化各种性质。我们生活在一个资源有限的世界里，人们一直致力于优化成本和速度的方法。 在软件工程方面而言，我认为，最流行的改善性能的就是缓存了。在许多app都有缓存，依赖于软件方面的存储，缓存背后的想法非常简单。为了加载较快，存储数据经常被用到。","text":"[译]合适以及为何使用最少使用(LFU)缓存与Golang中的实现在过去的这些年，参与计算机科学和工程师的人们一直在努力优化各种性质。我们生活在一个资源有限的世界里，人们一直致力于优化成本和速度的方法。 在软件工程方面而言，我认为，最流行的改善性能的就是缓存了。在许多app都有缓存，依赖于软件方面的存储，缓存背后的想法非常简单。为了加载较快，存储数据经常被用到。 事实上，缓存必须在两个方面很快 确保尽可能多的文件请求(缓存命中)，而不是通过网络或者主内存(没有命中) 使用它的开销应该比较小，测试人员决定何时更换文件 在这篇文章中，我们将会关注第二部分。对最不常用的缓存采取特定的实现方法，并使成员资格测试和驱逐算法具有良好的性能。并且，我们还将介绍基础知识并探究这种缓存方案可用的地方。 基础LFU是一种缓存算法。只要达到缓存的容量限制，就会删除缓存中最不常用项。这意味着对于缓存中的每个项目，我们必须跟踪它的使用频率。一旦超过了容量，讲运用驱逐算法，从缓存中挑选和过期（移除）项目。 如果你之前实现过LFU缓存，你可能已经考虑使用最小堆数据结构。因为它对数时间复杂度处理插入，删除和更新。在这篇文章中，我们将介绍另一种实现它的方法。 但在我们进入实施之前，让我们看看LFU在哪些情况下比替代品更好。 LFU闪耀点想象一下CDN上的资产缓存，其中资产根据使用模式进行缓存。因此，当用户在网页上请求加载一些图片时，此CDN会将其添加到缓存中，以便其他用户更快获取它。 例如,一个这样的图像(资产)是网站的标志，你能想象一天有多少次谷歌的标识被要求在他们的所有产品上。我真的很想找到这个数字，但就目前而言，我们可能会认同这个数字是庞大的。 这种资产缓存是LFU缓存的完美用例。LFU缓存逐出算法永远不会驱逐频繁访问的资产。事实上，在这样的缓存中，谷歌的微标几乎将永远缓存，相比之下。如果由于Reddit，Slashdot和Hackernews首页上的新产品的新登录页面而有任何图像将会访问，一旦超级风暴过去，资产将被驱逐得更快，因为访问频率将急剧下降，尽管在过去几天他们已经被访问过很多次。 正如你可能已经注意到的那样，在缓存对象的访问模式不经常更改的情况下。这种缓存逐出的方法非常有效。虽然LRU缓存将驱逐最近无法访问的资产，但LFU驱逐方法将在炒作结束后逐出不再需要的资产。 实现LFU缓存现在，让我们来了解它，如我们之前所说的。我们不是将min-heap视为可能支持LFU缓存的可能数据结构，而是考虑采用更好的方法。 事实上，在2010年，一组研究人员Ketan Shah教授，Anirban Mitra和Dhruv Matani发表了一篇题为“用户实现LFU缓存驱逐方案的O(1)算法”的文章（你可以点击这里查看），在这文章中他们解释LFU缓存的实现，其运行的时间复杂度为O（1）,用于其所有操作，包括插入，访问，和删除(驱逐)。 在此，我将向你展示如何实现此缓存并引导你完成实现。 数据结构不，它不会是某种科学怪人的红黑树，事实上，它是两个双向链表和一个哈希表。是的，就是这样。 为了能够理解LFU实现的基本原理，让我们将链表和哈希表看做图形。在我们查看实际图形之前，我们需要了解如何使用哈希表和链接列表。 哈希表将使用通过哈希算法处理的密匙存储所有项目(为了我们的目的，我们 可以保持简单)，值将是实际项目。 链表有点复杂，第一个将是”频率列表“，它将具有所有访问频率。此列表中的每一个节点都有一个项目列表。该列表将包含已使用相应频率访问的所有项目。此外，项目列表中的每一个项目都会在频率列表中指向其祖先。 如果我们查看上面的图形例子，我们可以注意到项A，B，C和D已被访问过一次。E和F项已被访问过4次，依据类推。蓝线是项列表中的每个项都与频率列表中的祖先有关的指针。 那么，如果再次访问项E会发生会发生什么？让我们完成以下步奏：1. 从哈希表中检索项很容易（并且很好地扩展）O（1）。 2. 我们将访问项的frequencyParent指针，从中我们可以检查列表中的下一个频率是什么。3. 如果存在新频率(列如8)，我们将其作为频率节点8下的项目列表的第一项。4. 如果新频率不存在，我们将创建频率节点8并将节点8添加E到项列表中. 就是这样，检索项并刷新项的频率是O（1）,在我们开始实现访问算法前，让我们首先建立我们需要的基本类型。 类型如我们之前所说，我们需要对所需的类型进行建模，这些类型将成为我们缓存的主干。 第一个结构将是CacheItem,这将是将存储在缓存中的实际项目。 123456type CacheItem struct &#123; key string // Key of entry value interface&#123;&#125; // Value of item frequencyParent *list.Element // Pointer to parent in cacheList&#125; 它包含我们可以在哈希表中查找它的键，值是实际的缓存项，以及指向频率列表中的frequencyParent指针。 下一个结构是FrequencyItem，它表示频率列表中的每一个项。它包含一组条目，这些条目将是一组CacheItem指针，我们将使用map来存储它，以便我们可以将其视为一个集合，它只包含唯一的项。 12345type FrequencyItem struct &#123; entries map[*CacheItem]byte // Set of entries freq int // Access frequency&#125; 我们需要具有平滑运行缓存的最后一个结构就是Cache本身。 1234567type Cache struct &#123; bykey map[string]*CacheItem // Hashmap containing *CacheItems for O(1) access freqs *list.List // Linked list of frequencies capacity int // Max number of items size int // Current size of cache&#125; Cache将包含hash键，称为bykey(命名来自上面链接的文件)，频率列表称为freqs，缓存的最大容量称为容量，缓存的大小表示任何给定缓存的项目数时刻。 New, set &amp; get让我们看看使缓存工作所需的前三个函数。第一个是一个小构造函数： 12345678910func New() *Cache &#123; cache := new(Cache) cache.bykey = make(map[string]*CacheItem) cache.freqs = list.New() cache.size = 0 cache.capacity = 100 return &amp;c&#125; 构造函数New将创建一个新的Cache结构，并将所有默认值设置为它。如果你想知道list.New（）是如何工作的：对于频率列表，我们将使用Go的容器/列表包，其中包含一个整洁的链表实现。你可以查看其文档以获取更多详细信息。 将在Cache上实现的第二个函数是Set函数： 123456789101112131415func (cache *Cache) Set(key string, value interface&#123;&#125;) &#123; if item, ok := cache.bykey[key]; ok &#123; item.value = value // Increment item access frequency here &#125; else &#123; item := new(CacheItem) item.key = key item.value = value cache.bykey[key] = item cache.size++ // Eviction, if needed // Increment item access frequency &#125;&#125; 该函数将缓存键和实际值/项缓存为参数。然后，它检查项目是否已经缓存。如果它被缓存，它只会更新项目的值。否则，它将创建一个新的CacheItem，它将封装实际值，它将设置密钥，它将把项添加到bykey哈希表，它将增加缓存的大小。 现在，在两个逻辑分支中，我为缺失的部分添加了一些注释：1。缓存必须知道如何增加aCacheItem的访问频率，但我们还没有实现它; 2.如果大小达到容量，缓存必须知道如何根据访问频率逐出项目。 我们将保留这些注释，直到我们实现增量和逐出函数。 Cache将接收的第三个函数是Get - 通过哈希表中的键访问项目并返回它： 123456789func (cache *Cache) Get(key string) interface&#123;&#125; &#123; if e, ok := cache.bykey[key]; ok &#123; // Increment acess frequency here return e.value &#125; return nil&#125; 这里也没有魔法 - 我们检查bykey散列表是否包含带有key参数的值，如果存在则返回它。否则，我们返回零。在这里，就像在Set中一样，我们将保留占位符注释，一旦我们实现它就必须添加频率增量函数调用。 更新访问频率正如我们已经看到的，对于缓存的每个访问操作，我们必须更新所访问项的访问频率。 让我们看一下我们的Increment函数必须采取的步骤。首先，对于要过期的项目，我们将不得不决定该项目是否已经是哈希表和频率列表的一部分。如果是，我们将不得不在频率列表中找到它的新频率值和下一个频率位置（节点）。 其次，我们必须弄清楚对于新频率，频率列表中是否已经存在节点。如果有，我们将不得不将该项添加到其条目列表中并分配其新的访问频率（即当前访问频率+ 1）。如果没有，我们将不得不在频率列表中创建一个新的频率节点（并设置其所有合理的默认值），然后将该项添加到其条目列表中 第三，一旦我们检测到FrequencyParent，我们的函数就必须将新的父项设置为正在递增的项，并将其添加到父项的列表中。 作为最后一步，增量函数将从旧频率节点（frequencyParent）的条目中删除该项目。 这是Golang代码： 12345678910111213141516171819202122232425262728293031func (cache *Cache) increment(item *CacheItem) &#123; currentFrequency := item.frequencyParent var nextFrequencyAmount int var nextFrequency *list.Element if currentFrequency == nil &#123; nextFrequencyAmount = 1 nextFrequency = cache.freqs.Front() &#125; else &#123; nextFrequencyAmount = currentFrequency.Value.(*FrequencyItem).freq + 1 nextFrequency = currentFrequency.Next() &#125; if nextFrequency == nil || nextFrequency.Value.(*FrequencyItem).freq != nextFrequencyAmount &#123; newFrequencyItem := new(FrequencyItem) newFrequencyItem.freq = nextFrequencyAmount newFrequencyItem.entries = make(map[*CacheItem]byte) if currentFrequency == nil &#123; nextFrequency = cache.freqs.PushFront(newFrequencyItem) &#125; else &#123; nextFrequency = cache.freqs.InsertAfter(newFrequencyItem, currentFrequency) &#125; &#125; item.frequencyParent = nextFrequency nextFrequency.Value.(*FrequencyItem).entries[item] = 1 if currentFrequency != nil &#123; cache.remove(currentFrequency, item) &#125;&#125; 让我们回顾一下频率和条目列表的原始图表，并逐步增加E项目。 我们的增量函数将采用的第一步是分配一个指向节点4（frequencyParent）及其值（即4）的指针。由于节点4存在于列表中，它将在频率列表中找到下一个节点，在我们的例子中是节点7。 一旦它确定E节点的新频率应为5而不是7，它将在节点4和7之间的列表中追加一个新的频率节点： 将5节点添加到列表后，该函数将设置节点正常运行所需的默认值。然后它会将E的指针设置为新的frequencyParent（5节点）： 作为最后一步，它将采用具有指针* CacheItem类型的项目，并将其添加到条目列表，同时从先前的frequencyParent的条目列表中删除它： 让我们看看从FrequencyItem的条目列表中删除CacheItem的步骤是什么。 删除条目一旦我们知道列表中我们想要删除它的节点，我们就可以从条目列表中删除该项，如果条目变空，还可以从频率列表中完全删除FrequencyItem： 12345678func (cache *Cache) Remove(listItem *list.Element, item *CacheItem) &#123; frequencyItem := listItem.Value.(*FrequencyItem) delete(frequencyItem.entries, item) if len(frequencyItem.entries) == 0 &#123; cache.freqs.Remove(listItem) &#125;&#125; 驱逐拼图的最后一部分是逐出，或者换句话说，一旦缓存达到其最大容量，就删除最不常用的项目。 我们必须知道我们想要驱逐多少项。通常，我们的缓存将具有低限和高限，因此当达到上限时，我们将删除项目直到下限。在我们的例子中，我们将驱逐任意数量的项目，Evict函数将作为参数。 该功能将从开始到结束开始“遍历”频率列表。由于频率列表是按升序排列的，因此它将开始从第一个频率节点开始删除条目，直到它删除与传入的任意数字一样多的项目。 如果频率节点由于逐出而不包含条目，则Evict函数也必须从频率列表中移除频率节点。它将通过调用Remove函数来实现。这样，驱逐就不会留下任何垃圾。 这是我们上面描述的代码： 123456789101112131415func (cache *Cache) Evict(count int) &#123; for i := 0; i &lt; count; &#123; if item := cache.freqs.Front(); item != nil &#123; for entry, _ := range item.Value.(*FrequencyItem).entries &#123; if i &lt; count &#123; delete(cache.bykey, entry.key) cache.Remove(item, entry) cache.size-- i++ &#125; &#125; &#125; &#125;&#125; 回到Set and Get在本文开头，我们实现了Set和Get函数。那时我们没有的东西是Evict和increment函数，所以我们可以相应地使用它们。让我们添加他们的调用。 增加访问频率在Get函数中，如果我们在bykey哈希表中找到一个项目，我们需要在继续返回其值之前增加它的访问频率： 123456789101112131415161718192021222324252627282930func (cache *Cache) Get(key string) interface&#123;&#125; &#123; if e, ok := cache.bykey[key]; ok &#123; cache.increment(e) return e.value &#125; return nil&#125;``通过此更改，Cache将在返回之前增加该特定项的频率。但是，我们忘了什么吗？此外，Set函数在实际缓存它们时访问缓存的项目。这意味着当一个项被缓存时，它将立即被添加到频率列表中，值为1的节点下：```gofunc (cache *Cache) Set(key string, value interface&#123;&#125;) &#123; if item, ok := cache.bykey[key]; ok &#123; item.value = value cache.increment(item) &#125; else &#123; item := new(CacheItem) item.key = key item.value = value cache.bykey[key] = item cache.size++ // Eviction, if needed cache.increment(item) &#125;&#125; 在驱逐后Set函数允许我们的LFU Cache用户在其中缓存更多项目。任何缓存的一个关键组件是，当新项目添加到缓存时，它应该知道如何逐出项目（释放空间）。对于LFU缓存，当缓存达到容量时，需要删除最不常用的项。 让我们首先添加一个函数，如果缓存达到其最大容量，它将返回一个bool： 1234func (cache *Cache) atCapacity() bool &#123; return cache.size &gt;= cache.capacity&#125; 功能很简单：检查Cache的当前大小是大于还是等于容量。 现在，让我们在Set函数中使用它。一旦我们在缓存中设置了新项目，我们就必须检查缓存是否已达到其容量，然后从中删除多个项目。 为简单起见，我们每次达到最大容量时只会删除10个项目： 1234567891011121314151617func (cache *Cache) Set(key string, value interface&#123;&#125;) &#123; if item, ok := cache.bykey[key]; ok &#123; item.value = value cache.increment(item) &#125; else &#123; item := new(CacheItem) item.key = key item.value = value cache.bykey[key] = item cache.size++ if cache.atCapacity() &#123; cache.Evict(10) &#125; cache.increment(item) &#125;&#125; 通过此更改，如果在任何时候添加项目达到缓存的容量，缓存将驱逐最不常用的项目。 通过此更改，如果在任何时候添加项目达到缓存的容量，缓存将驱逐最不常用的项目。 如果您想查看本文的完整代码，可以查看这 关于缩放和时间复杂性的评论LFU是一个有趣的驱逐计划，特别是与LRU相比，在我看来，由于其非常规性质。虽然其应用受到限制，但由于该方法的扩展能力，本文中使用的论文中解释的算法和后备数据结构非常吸引人。 如果我们重新阅读本文开头提到的论文，我们将看到虽然LFU不是新闻，但它传统上是使用min-heap实现的，它具有插入，查找和删除的对数时间。有趣的是，在本文中，作者解释说，他们提出的方法对于每个操作（插入，查找和删除）都具有O（1）时间复杂度，因为操作基于哈希表。此外，链接列表不会增加任何时间复杂度，因为我们不会在任何时候遍历列表 - 我们只是在需要时添加或删除其中的节点（这是一个O（1）操作）。 总结在本文中，我们了解了LFU缓存的基础知识。我们确定了最重要的绩效指标（命中率，成员资格和驱逐速度）。我们看到虽然它不是最广泛使用的缓存方案，但在某些用例中肯定会非常高效。 然后我们继续实施它，使用一种在时间复杂度方面可以很好地扩展的方法。我们看到了实施驱逐和频率增量算法的复杂性。最后，我们进一步探讨了我们用于实现它的方法如何扩展。 如果您想阅读有关该主题的更多信息，请参阅以下几个链接，以丰富您对LFU缓存和缓存的了解： “An O(1) algorithm for implementing the LFU cache eviction scheme”- Prof. Ketan Shah, Anirban Mitra, Dhruv Matani “Caching in theory and practice”- Pavel Panchekha “LFU (Least Frequently Used) Cache Implementation”- Geeks for Geeks 本文翻译自–原文","categories":[],"tags":[{"name":"golang","slug":"golang","permalink":"https://www.liangyouze.com/tags/golang/"}]},{"title":"vim技巧","slug":"vim 技巧","date":"2019-06-01T06:10:23.000Z","updated":"2019-06-01T07:14:16.238Z","comments":true,"path":"2019/06/01/vim 技巧/","link":"","permalink":"https://www.liangyouze.com/2019/06/01/vim 技巧/","excerpt":"目录简介小技巧启动及关闭教程篇文本编辑文本编辑的高效命令other 简介得益于 vim 的指法，敲起代码来如行云流水。不管是不是写代码，学好vim 指法相当重要，当然最重要的还是为了效率，节省时间做更多其他的事。","text":"目录简介小技巧启动及关闭教程篇文本编辑文本编辑的高效命令other 简介得益于 vim 的指法，敲起代码来如行云流水。不管是不是写代码，学好vim 指法相当重要，当然最重要的还是为了效率，节省时间做更多其他的事。 小技巧“工欲善其事，必先利其器”。在 Vi/Vim 版本的选择上，原则是“能用 Vim 就不要使用 Vi”。Vim 提供的功能和特性要比 Vi 多得多，如语法加亮着色功能等。就使用效果及效率来说，编辑同样的文件，使用 Vim 更胜一筹；就版本来说，新版的往往会修复旧版的一些缺陷及不足。这就要求我们在可能的情况下一定要使用最新版的 Vim。 启动及关闭 退出 ZQ 无条件退出 q!无条件退出 ZZ 存盘并退出 :wq 存盘并退出 保存部分文件 :m,nw &lt; file&gt;将 m 行到 n 行部分的内容保存到文件 中 :m,nw &gt;&gt; 将 m 行到 n 行的内容添加到文件 的末尾 保存文件 :w 教程篇默认的 vim 是没有显示行数的，可自行在 vim 配置文件里开启(自行Google) Vi/Vim 中操作单位有很多，按从小到大的顺序为（括号内为相应的操作命令）：字符（h、l）→ 单词 (w、W、b、B、e) → 行 (j、k、0、^、$、:n) → 句子（(、)）→ 段落（{、}）→ 屏 (H、M、L) → 页（Ctrl-f、Ctrl-b、Ctrl-u、Ctrl-d) → 文件（G、gg、:0、:$）。 字符 h左移一位,l右移一位 单词 w/W 移动到下一单词的开头 b/B 移动到上一单词的开头 e/E 移动到光标所在单词的末尾 f 快速移动到下一个字符的位置 行 j 下移一行 k 上移一行 0 移到当前行开头 ^ 移到当前行的第一个非空字符 $ 移到当前行末尾 :n 移动到第 n 行 句子 ) 移动到当前句子的末尾 ( 移动到当前句子的开头 段落 } 移动当前段落的末尾 { 移到当前段落的开头 屏 H 移动到屏幕的第一行 M 移动到屏幕的中间一行 L 移动到屏幕的最后一行 页 Ctrl-f 向前滚动一页 Ctrl-b 向后滚动一页 Ctrl-u向前滚动半页 Ctrl-d 向后滚动半页 文件 G 移动到文件末尾 gg 移动到文件开头 :0移动到文件第一行 :$ 移动到文件最后一行 文本编辑 与光标移动一样，Vi/Vim 中关于编辑操作的命令也比较多，但操作单位要比移动光标少得多。按从小到大的顺序为（括号内为相应的操作命令）：字符 （x、c、s、r、i、a）→ 单词 (cw、cW、cb、cB、dw、dW、db、dB) → 行 (dd、d0、d$、I、A、o、O) → 句子（(、)）→ 段落（{、}）。这些操作单位有些可以加操作次数。操作对象的范围计算公式为：操作范围 = 操作次数 * 操作单位。比如：d3w 命令删除三个单词，10dd 命令删除十行。 字符 x 删除光标位置的字符 c 更改当前字符并进入插入模式 s 替换光标位置的字符并进入插入模式 r 替换光标位置的字符但不进入插入模式 i 在当前位置的字符之前进入插入模式 a 在当前位置的字符之后进入插入模式 单词 cw/cW 删除当前单词从光标开始的部分并进入插入模式 cb/cB 删除当前单词从光标所在位置至单词开始的部分并进入插入模式 dw/dW 删除当前单词从光标开始的部分但不进入插入模式 db/dB 删除当前单词从光标所在位置至单词开始的部分但不进入插入模式 行 dd 删除当前行 d0 删除从当前光标开始到行末的内容 d$ 删除从当前光标开始到行末的内容 I 在当前行的行首进入插入模式 A 在当前行的行尾进入插入模式 o 在当前行下方另起一行进入插入模式 O 在当前行上方另起一行进入插入模式 句子 d) 删除当前句子从光标位置开始到句末的内容 d( 删除当前句子从光标位置开始到句首的内容 段落 d} 删除当前段落从光标位置开始到段末的内容 d{ 删除当前段落从光标位置开始到段首的内容 文本编辑的高效命令 复制与粘贴 yw 复制当前单词从光标开始的部分 yy 复制光标所在行的所有字符 p 将最后一个删除或复制文本放在当前字符 P 将最后一个删除或复制文本放在当前字符之前 撤消与重做 u 撤消更改 Ctrl-R 重做更改 重复操作 .重复上次操作 交换相邻字符或行 xp 交换光标位置的字符和它右边的字符 ddp 交换光标位置的行和它的下一行 大小写转换 ~ 将光标下的字母大小写反向转换 guw 将光标所在的单词变为小写 guw 将光标所在的单词变为小写 gUw 将光标所在的单词变为大写 guu 光标所在的行所有字符变为小写 gUU 光标所在的行所有字符变为大写 g~~ 光标所在的行所有字符大小写反向转换 排序 :1,$!sort 将文件内的所有内容排序 other先定单位再定量 操作对象的范围计算公式为：操作范围 = 操作次数 * 操作单位。比如：5h 命令左移 5 个字符，8w 命令右移 8 个单词。","categories":[{"name":"vim","slug":"vim","permalink":"https://www.liangyouze.com/categories/vim/"}],"tags":[{"name":"vim","slug":"vim","permalink":"https://www.liangyouze.com/tags/vim/"}]},{"title":"golang-defer","slug":"defer","date":"2019-01-11T06:10:23.000Z","updated":"2019-08-17T06:20:02.211Z","comments":true,"path":"2019/01/11/defer/","link":"","permalink":"https://www.liangyouze.com/2019/01/11/defer/","excerpt":"defer的使用特点其实其中一点特性我理解起来就有点像java中的finally的用法 关于官方解释 123A defer statement defers the execution of a function until the surrounding function returns.The deferred call's arguments are evaluated immediately, but the function call is not executed until the surrounding function returns. 这里提到了defer调用的参数会立即计算，但在周围函数返回之前不会执行函数调用。","text":"defer的使用特点其实其中一点特性我理解起来就有点像java中的finally的用法 关于官方解释 123A defer statement defers the execution of a function until the surrounding function returns.The deferred call's arguments are evaluated immediately, but the function call is not executed until the surrounding function returns. 这里提到了defer调用的参数会立即计算，但在周围函数返回之前不会执行函数调用。 以及延迟函数调用被压入堆栈。当函数返回时，其延迟调用以后进先出顺序执行。 它有如何特点 所在的函数中，它在 return 或 panic 或 执行完毕 后被调用 多个 defer，它们的被调用顺序，为栈的形式。先进后出，先定义的后被调用 看下面几个例子： 在计算defer语句时，将计算延迟函数的参数。在此示例中，在延迟Println调用时计算表达式“i”。函数返回后，延迟调用将打印“0”。 123456func a() &#123; i := 0 defer fmt.Println(i) i++ return&#125; 在周围函数返回后，延迟函数调用以后进先出顺序执行。 12345func b() &#123; for i := 0; i &lt; 4; i++ &#123; defer fmt.Print(i) &#125;&#125; //将会打印3210 然后不免在使用过程中会遇到这些坑 坑1. defer在匿名返回值和命名返回值函数中的不同表现 12345678910111213141516func returnValues() int &#123; var result int defer func() &#123; result++ fmt.Println(\"defer\") &#125;() return result&#125;func namedReturnValues() (result int) &#123; defer func() &#123; result++ fmt.Println(\"defer\") &#125;() return result&#125; &nbsp;&nbsp;上面的方法会输出0，下面的方法输出1。上面的方法使用了匿名返回值，下面的使用了命名返回值，除此之外其他的逻辑均相同，为什么输出的结果会有区别呢？ &nbsp;&nbsp;要搞清这个问题首先需要了解defer的执行逻辑，defer语句在方法返回“时”触发，也就是说return和defer是“同时”执行的。以匿名返回值方法举例，过程如下。 将result赋值给返回值（可以理解成Go自动创建了一个返回值retValue，相当于执行retValue = result） 然后检查是否有defer，如果有则执行 返回刚才创建的返回值（retValue） 在这种情况下，defer中的修改是对result执行的，而不是retValue，所以defer返回的依然是retValue。在命名返回值方法中，由于返回值在方法定义时已经被定义，所以没有创建retValue的过程，result就是retValue，defer对于result的修改也会被直接返回。 坑2. 判断执行没有err之后，再defer释放资源 一些获取资源的操作可能会返回err参数，我们可以选择忽略返回的err参数，但是如果要使用defer进行延迟释放的的话，需要在使用defer之前先判断是否存在err，如果资源没有获取成功，即没有必要也不应该再对资源执行释放操作。如果不判断获取资源是否成功就执行释放操作的话，还有可能导致释放方法执行错误。 正确做法 1234567resp, err := http.Get(url)// 先判断操作是否成功if err != nil &#123; return err&#125;// 如果操作成功，再进行Close操作defer resp.Body.Close() 坑3. 调用os.Exit时defer不会被执行当发生panic时，所在goroutine的所有defer会被执行，但是当调用os.Exit()方法退出程序时，defer并不会被执行。 123456func deferExit() &#123; defer func() &#123; fmt.Println(\"defer\") &#125;() os.Exit(0)&#125; 上面的defer并不会输出。 坑4.非引用传参给defer调用的函数，且为非闭包函数，值不会受后面的改变影响 123456func defer0() &#123; a := 1 // a 作为演示的参数 defer fmt.Println(a) // 非引用传参，非闭包函数中，a 的值 不会 受后面的改变影响 a = a + 2&#125;// 控制台输出 1 坑5. 传递引用给defer调用的函数，即使不使用闭包函数，值也会受后面的改变影响 12345678910func myPrintln(point *int) &#123; fmt.Println(*point) // 输出引用所指向的值&#125;func defer1() &#123; a := 3 // &amp;a 是 a 的引用。内存中的形式： 0x .... ---&gt; 3 defer myPrintln(&amp;a) // 传递引用给函数，即使不使用闭包函数，值 会 受后面的改变影响 a = a + 2&#125;// 控制台输出 5 坑6. 传递值给defer调用的函数，且非闭包函数，值不会受后面的改变影响 12345678910func p(a int) &#123; fmt.Println(a)&#125;func defer2() &#123; a := 3 defer p(a) // 传递值给函数，且非闭包函数，值 不会 受后面的改变影响 a = a + 2&#125;// 控制台输出： 3 坑7. defer调用闭包函数，且内调用外部非传参进来的变量，值会受后面的改变影响 123456789// 闭包函数内，事实是该值的引用func defer3() &#123; a := 3 defer func() &#123; fmt.Println(a) // 闭包函数内调用外部非传参进来的变量，事实是该值的引用，值 会 受后面的改变影响 &#125;() a = a + 2 // 3 + 2 = 5&#125;// 控制台输出： 5 坑8. defer调用闭包函数，若内部使用了传参参数的值。使用的是值 123456789101112131415func defer5() &#123; a := []int&#123;1,2,3&#125; for i:=0;i&lt;len(a);i++ &#123; // 闭包函数内部使用传参参数的值。内部的值为传参的值。同时引用是不同的 defer func(index int) &#123; // index 有一个新地址指向它 fmt.Println(a[index]) // index == i &#125;(i) // 后进先出，3 2 1 &#125;&#125;// 控制台输出： // 3// 2// 1 坑9. defer所调用的非闭包函数，参数如果是函数，会按顺序先执行（函数参数） 1234567891011121314151617181920func calc(index string, a, b int) int &#123; ret := a + b fmt.Println(index, a, b, ret) return ret&#125;func defer6() &#123; a := 1 b := 2 // calc 充当了函数中的函数参数。即使在 defer 的函数中，它作为函数参数，定义的时候也会首先调用函数进行求值 // 按照正常的顺序，calc(\"10\", a, b) 首先被调用求值。calc(\"122\", a, b) 排第二被调用 defer calc(\"1\", a, calc(\"10\", a, b)) defer calc(\"12\",a, calc(\"122\", a, b))&#125;// 控制台输出：/**10 1 2 3 // 第一个函数参数122 1 2 3 // 第二个函数参数12 1 3 4 // 倒数第一个 calc1 1 3 4 // 倒数第二个 calc*/ 注意 defer 不影响 return的值 参考1 参考2","categories":[{"name":"golang","slug":"golang","permalink":"https://www.liangyouze.com/categories/golang/"}],"tags":[{"name":"golang","slug":"golang","permalink":"https://www.liangyouze.com/tags/golang/"}]},{"title":"2018年总结","slug":"2018总结","date":"2018-12-30T17:10:23.000Z","updated":"2018-12-30T18:14:11.000Z","comments":true,"path":"2018/12/31/2018总结/","link":"","permalink":"https://www.liangyouze.com/2018/12/31/2018总结/","excerpt":"帝都的生活&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;依稀记得在18年初的时候，毅然决定从重庆离职，离开这个舒适区，因为我明白，在这边待越久，差距就会越来越大，等到过完年来帝都搬砖。来到这边才感知和重庆的天壤之别。","text":"帝都的生活&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;依稀记得在18年初的时候，毅然决定从重庆离职，离开这个舒适区，因为我明白，在这边待越久，差距就会越来越大，等到过完年来帝都搬砖。来到这边才感知和重庆的天壤之别。每天早上13号线挤得不要不要的，但是9点后出门的话人就会少很多。正是因为这边上班晚，所以下班就迟了。加班也是常态。其实正常的工作日，加上通勤的时间，自己的时间久真的不太多。周末，和朋友们去过故宫，是挺大挺庄严的。但其实这些建筑之类的，去过一次也就够了。相比于去过的圆明园，更加喜欢这种自然风的恬静。北方是山也不算高，从故宫后面的景山看整个故宫挺震撼的。以及珍藏奇珍异宝的国家博物馆，逛了足足半天。只能说中外文物意义非凡，而且看着古代匠人的艺术感挺强的，因为不懂技术的我其实很想多认识一些“艺术”朋友。当深夜行走在奥体公园的时候，雨滴在五彩斑斓的灯光勾勒下仿佛也有一种彩虹的错觉。团建的时候开过的卡丁卡，到现在也还记得速度下的刺激。盛夏时节去十渡也有一种暴晒的感觉，当然对比重庆来说还是要好很多。在帝都工作是很忙，忙里偷闲也不缺乏去看看其他不一样的世界。由于一天都是坐办公室内，再加上每天良好的伙食不知不觉中就长胖了，意识到每天不到3000的步数会导致我的身体体能下降。本着在学校练出的毅力办了健身卡，3个月就再也没去过了。很大一部分原因是公司项目真的很忙吧。4月的时候竟然还在北京见到了雪，虽然不足为奇，但感受到南北方的气候的初步差异。不久后就到了5月，答辩的日子。也没有留恋学校的日子， 所以呆了10天就走了。在这期间很感谢我的导师zy。毕业不是再见，是更好的遇见。北京的空气是真的干，动不动就是静电电你。还有就是抗寒能力不错的我在刺骨的冬天也真的觉得冷哇，不敢拿身体开玩笑。 帝都的工作&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;从到这边来就感受到了和重庆的技术有这天壤之别，技术层出不穷，但就国内而言前沿的技术还是先在北上广这些城市试用。互联网的人来人往，流动性大。但其实对于加班这点来看，确实会因为菜的原因加班，再就是就算不加班，也不喜欢早走，因为还可以在公司继续学习，可能氛围比在家好些。因为各种原因，在10月的时候换到了现在这家公司。说实话挺感谢我对面的同事zjf,虽然在技术成长的方面靠自己，但是在其他事情上，比如业务还是很耐心的给我讲，挺好的一个人。以及旁边同事就比我大一届，如果我和他是同届也被他甩了几条街了吧。当然要学习的还有很多很多，在工作中也有很多时候效率低下，开发相关知识肯定是基础，然后也一个人的思维能力可能就真的是发展的天花板。这点还要不断加强练习。很快就在这边呆了一年了，还是先干好手头上的事情。来这边你会知道，优秀的人比你还努力很多倍。不过好在现在终于算稳定了吧，从实习开始主语言就从c#换成后面的Java和js并行。然后换到了现在的golang。 其他原本搭建这个博客是为了更好地将自己的吸收转为输出，但写着写着回过头来看就觉得是太简单的，没啥必要记录。以及自己掌握的东西还不够，怕误导其他人。所以就先决定再沉淀一段时间。回头看看，今年好像就做了这些数字相关的吧。 参加3场技术沙龙 在帝都的这一年，参加了3场感兴趣的技术沙龙，但也有其他原因错失了我想去的。参加这些技术分享会的动机很单纯，就是去学习其他前沿技术。有以下几点好处 认识一些周末不打游戏，不陪妹子的人。对技术充满热情的人。 不同公司会有一定得局限性，比如上家公司我做的那个项目es索引都才几个，其他公大公司都是搜索一个团队，对于海量的数据，如何如何处理。也正是我去了es社区的分享后很想做es方面的工作。但是现在这个机器人团队也挺好的。 因为目前很多知识都还是属于懵懂阶段，对眼界的开阔的一定帮助。 认识一些大牛，其实并不是为了大牛能够帮你解决疑难问题，有的时候你加上别人，看看别人的朋友圈，说不定也能掌握业内的一些动向。 读完14本书 只能说通勤的时间很充裕 看完28部电影 当然几乎都是电脑上看的。相比去年的110部很满足了。 明年争取减少到10部","categories":[{"name":"杂谈","slug":"杂谈","permalink":"https://www.liangyouze.com/categories/杂谈/"}],"tags":[{"name":"杂谈","slug":"杂谈","permalink":"https://www.liangyouze.com/tags/杂谈/"}]},{"title":"我所读过的书","slug":"我所读过的书","date":"2018-10-10T12:10:33.000Z","updated":"2018-12-09T16:25:54.000Z","comments":true,"path":"2018/10/10/我所读过的书/","link":"","permalink":"https://www.liangyouze.com/2018/10/10/我所读过的书/","excerpt":"这里记录一下关于我读过的书籍","text":"这里记录一下关于我读过的书籍 2018年 Java JDK 7学习笔记 时间: 2018年1月18日 18:42:27 很久之前看的了。突然想标记一下。初学的话就花2h看一下吧。不太建议看的书 高效程序员的45个习惯：敏捷开发修炼之道 时间：2018年3月9日09:45:14 可能是现在还不是很注重，所以很快就浏览完了。我感觉工作半年内还是不读的好，这本书的价值不大 Mongo基础命令参考 时间：2018年3月18日23:11:02 一本野书，基本上就是熟悉一些Ｍｏｎｇｏｄｂ的命令，可以花１个小时浏览下就完了 JAVA编程思想 时间：2018年4月17日00:26:07 说实话，这本书真的是太长了，有精华的东西，同时也有淘汰的东西。距离上次记录书籍已经有一个月了，等再看的时候详细看一遍吧。 算法图解 时间：2018年5月14日00:41:01 Aditya Bhargava 作品，很不错的一本书，再加上是我非常喜欢的图灵教育出版的书。推荐读，而且讲的很形象。基础算法 人工智能 时间：2018年06月26日20:21:4５ 李开复的作品，对认知又多了一点，反正我是很相信人工智能带给社会的进步，以后必将是高科技与艺术的并存。工业上可以取代，但艺术不能取代。 Redis 入门指南 时间：2018年7月31日22:51:45 李子骅 编著 很好的一本基础书，命令很多。记了一些常用的。 Go语言编程 时间：2018年11月18日20:05:42 作者：许式伟 吕桂华 有点失望，给我的感觉就好像是Java一些所谓的“从入门到精通”一类的书，300页的书，看看目录其实有的人就会选择不看，但我还是看完了。深度也是点到为止，当然这其实是要自己去挖掘的。网络编程的那些其实我感觉就是翻译一些手册，也不是国外的文章。比较偏向于教科书方面的书 数学之美 时间：2018年12月06日09:22:58 作者：吴军 马尔可夫链是如此的熟悉，常见的新闻分类竟然是利用了余弦函数。其实大学阶段的知识真的是基础，比如说自然语言处理其实就可以抽象为比较简单的通信模型和统计学模型。利用一些概率公式然后再加上马尔科夫假设就可以做到机器翻译和语音识别。以及我现阶段最想做的搜索。其实布尔代数在支撑着搜索引擎索引的数学基础，当然要做好每个方向是要掌握本质以及精髓，做起事情来也会如诗般顺滑。 2017年 java解惑 这本书还好吧，反正让你会怀疑你的基础学的不扎实，新手老手都建议看一看，不过快速的看一下就好，花1天时间吧 图解HTTP 很好的一本书，值得一读 Java核心技术+卷1（原书第9版）》 这本书粗略过了，因为看的是pdf，打算再吧第十版看下 代码整洁之道 业界传闻的巴拉巴拉的必读书，同样建议看一看，好的代码格式，好的代码规范，以及注释等，也能体现一个人的水平。以前面试阿里的时候，就被问到代码规范之类的问题。建议多刻意练习。多看点源码，源码的规范就很好。","categories":[],"tags":[{"name":"读书","slug":"读书","permalink":"https://www.liangyouze.com/tags/读书/"}]},{"title":"elasticsearch社区分享会","slug":"elasticsearch社区分享会","date":"2018-09-11T06:10:23.000Z","updated":"2018-10-08T05:46:01.000Z","comments":true,"path":"2018/09/11/elasticsearch社区分享会/","link":"","permalink":"https://www.liangyouze.com/2018/09/11/elasticsearch社区分享会/","excerpt":"在前段时间加班的时候错过了两场我想去的技术会，这次终于没落空了。大佬也多，涨了不少姿势。特此记录一下分享，由于全凭记忆叙述，可能就没啥顺序而言的还原出之前的收获。","text":"在前段时间加班的时候错过了两场我想去的技术会，这次终于没落空了。大佬也多，涨了不少姿势。特此记录一下分享，由于全凭记忆叙述，可能就没啥顺序而言的还原出之前的收获。 确实目前项目中目前涉及到了elasticsearch不多，索引都才几个。看到别人分享的都是2千，4-5千的索引量。而且数据量大的话才更能体现出elasticsearch的作用。 周金阳 果壳网/在行 算法工程师 算法果然是大佬，让es与深度学习结合起来在搜索这块已经走在很多公司的前面了吧。 使用 ES 来构建一个简易却行之有效的个性化推荐系统，以及一些高级搜索排序的实践。 搜索排序主要是分享一些机器学习工具与 ES 配合的实践心得。 思考一个问题，如果是这样的你会选择怎么排序1234&#123; \"title\":\"引力波\" \"content\":\"引力波引力波引力波\"&#125; 1234&#123; \"title\":\"引力波,一个世纪的求索\" \"content\":\"在物理学中，引力波是指时空弯曲中的涟漪，通过波的形式从辐射源向外传播，这种波以引力辐射的形式传输能量。在1916年，爱因斯坦基于广义相对论预言了引力波的存在。引力波的存在是广义相对论洛伦兹不变性的结果，因为它引入了相互作用的传播速度有限的概念。相比之下，引力波不能够存在于牛顿的经典引力理论当中，因为牛顿的经典理论假设物质的相互作用传播是速度无限的。\"&#125; 若输入的值和被检索到的结果呈线性变化g(q,x)很明显，第一个是用户测试的或者是胡乱写的，当用户输入“引力波”的时候，如何控制类似于这种情况让正常的显示在前。这种情况，就可以加一些其他的限制条件f(x),比如1得出来的期望值为15.42,2得出来的期望值为87.93，这样关于g(q,x) -&gt; f(x)*g(q,x) 当然如果要做的好的话需要优化的还有很多，比如用BiLSTM+CNN 期望后期会用到这些吧，毕竟我觉得这是偏离业务而且是和大数据接轨的之一。 其中，在使用es的时候有一些规范和约束， 业务索引尽量自定义id，数据敏感业务自备插入修改时间 一个索引一个type 控制单次搜索结果条数，总条数由es限制。控制请求超时时间 关于es的使用也有在调用链日志 一个节点一个主分片，0副本， 批量写入，控制单批写入字节数 在生产阶段，调用链日志写入慢，kafka会出现大量堆积等现象，关于如何解决。有以下方案， 索引写入时会伴随着id校验，请求体解析，分词等操作，都会带来一定的cpu开销。原先的索引结构中存在部分多余字段，无需进行分词，取消后可以减轻cpu压力。 使用es自动生成id，省去id检查步骤。调整translog合并时间，半小时一次，防止过多merge任务导致cpu开销过大。 在业务索引随着场景变化，写入量逐渐增多，集群cpu load变高，原来单个主分片写入出现瓶颈遇到这种情况 可以重建索引，主分片改为2个，分别分布在两台机器，达到负载均衡效果，数据需要迁移。 在提及到es时，不得不说也是和spark相关。这里就不展开讲了，下次深入了解的时候再学习。 番外大公司都是搜索是一个团队，虽然我业务写的也不好，但是我更倾向于这种方向。分享者都很强，有开发相关的以及运维，技术演变快，找准自己的一个兴趣点，专研下去。","categories":[],"tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"https://www.liangyouze.com/tags/elasticsearch/"}]},{"title":"关于生成订单号规则的一些思考","slug":"关于生成订单号规则的一些思考","date":"2018-08-10T12:10:33.000Z","updated":"2018-10-08T02:49:57.000Z","comments":true,"path":"2018/08/10/关于生成订单号规则的一些思考/","link":"","permalink":"https://www.liangyouze.com/2018/08/10/关于生成订单号规则的一些思考/","excerpt":"关于我为什么写这篇文章是因为今天在做订单模块的时候,看到之前的PRD上描述的订单生成规则是由 年月日＋用户id2位+企业id位＋四位自增长数。然后竟被我反驳的突然改成了精确时间＋4位自增长数，于是我更失望了。 我们考虑一下，据我所常见的订单基本都14-20位。(年月日时分秒和随机数)基本上就有14位了。虽然一般项目做不到淘宝双11这种支付峰值达到每秒10万笔订单.但是我觉得至少事先可以考虑到，想必当初淘宝或许也没意识到以后发展得这么好。","text":"关于我为什么写这篇文章是因为今天在做订单模块的时候,看到之前的PRD上描述的订单生成规则是由 年月日＋用户id2位+企业id位＋四位自增长数。然后竟被我反驳的突然改成了精确时间＋4位自增长数，于是我更失望了。 我们考虑一下，据我所常见的订单基本都14-20位。(年月日时分秒和随机数)基本上就有14位了。虽然一般项目做不到淘宝双11这种支付峰值达到每秒10万笔订单.但是我觉得至少事先可以考虑到，想必当初淘宝或许也没意识到以后发展得这么好。 背景为了达到业务订单的生成。我觉得要至少要符合以下这三种, 全局唯一 一定不能重复 在复杂的分布式系统中，很多场景需要的都是全局唯一ID的场景，一般为了防止冲突可以考虑的有36位的UUID,twitter的snowflake等。 但是可以思考这些问题？ 是不是应该有一些其他意义的思考，比如说订单系统有买家的id(取固定几位) 是否有商品的标识,方便熟悉业务的排查问题或者查询也通过不去系统查找可以有个初步的认识，但是业务量大的话感觉就可以排除这个人为的去辨识了。 个人的看法是主要是唯一，其他关于业务方面的不是太太重要。 查阅了相关资料，主要有以下这几种 UUID, 组成：当前日期+时间+时钟序列+机器识别号（Mac地址或其他）没有mac网卡的话会有别的东西识别。 在分布式系统中，所有元素（WEB服务器）都不需要通过中央控制端来判断数据唯一性。几十年之内可以达到全球唯一性。 snowflake的结构如下(每部分用-分开): Mysql通过AUTO_INCREMENT实现、Oracle通过Sequence序列实现。在数据库集群环境下，不同数据库节点可设置不同起步值、相同步长来实现集群下生产全局唯一、递增ID Snowflake算法 雪花算法 41位时间戳+10位机器ID+12位序列号（自增） 转化长度为18位的长整型。 Twitter为满足美秒上万条消息的创建，且ID需要趋势递增，方便客户端排序。 Snowflake虽然有同步锁，但是比uuid效率高。 Redis自增ID 实现了incr(key)用于将key的值递增1，并返回结果。如果key不存在，创建默认并赋值为0。 具有原子性，保证在并发的时候。 但是我在这主要想说的是雪花算法生成id,至于为什么，就测试了一下其他的，感觉这种生成方式个人比较喜欢。 Snowflake算法规则如下 使用41bit作为毫秒数，10bit作为机器的ID（5个bit是数据中心，5个bit的机器ID），12bit作为毫秒内的流水号（意味着每个节点在每毫秒可以产生 4096 个 ID），最后还有一个符号位，永远是0。 0 - 0000000000 0000000000 0000000000 0000000000 0 - 00000 - 00000 - 000000000000 该算法实现基本是二进制操作。 一共加起来刚好64位，为一个Long型。(转换成字符串长度为18) snowflake生成的ID整体上按照时间自增排序，并且整个分布式系统内不会产生ID碰撞（由datacenter和workerId作区分），并且效率较高。据说：snowflake每秒能够产生26万个ID。 以下是代码部分借鉴与网络100万个ID 耗时２秒123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129/** * Created by youze on 18-7-5 */public class IdWorker &#123; /** * 起始的时间戳 */ private final static long START_STMP = 1530795377086L; /** * 每一部分占用的位数 */ /** * 序列号占用的位数 */ private final static long SEQUENCE_BIT = 12; /** * 机器标识占用的位数 */ private final static long MACHINE_BIT = 5; /** * 数据中心占用的位数 */ private final static long DATACENTER_BIT = 5; /** * 每一部分的最大值 */ private final static long MAX_DATACENTER_NUM = -1L ^ (-1L &lt;&lt; DATACENTER_BIT); private final static long MAX_MACHINE_NUM = -1L ^ (-1L &lt;&lt; MACHINE_BIT); private final static long MAX_SEQUENCE = -1L ^ (-1L &lt;&lt; SEQUENCE_BIT); /** * 每一部分向左的位移 */ private final static long MACHINE_LEFT = SEQUENCE_BIT; private final static long DATACENTER_LEFT = SEQUENCE_BIT + MACHINE_BIT; private final static long TIMESTMP_LEFT = DATACENTER_LEFT + DATACENTER_BIT; /** * 数据中心 */ private long datacenterId; /** * 机器标识 */ private long machineId; /** * 序列号 */ private long sequence = 0L; /** * 上一次时间戳 */ private long lastStmp = -1L; public IdWorker(long datacenterId, long machineId) &#123; if (datacenterId &gt; MAX_DATACENTER_NUM || datacenterId &lt; 0) &#123; throw new IllegalArgumentException(&quot;datacenterId can&apos;t be greater than MAX_DATACENTER_NUM or less than 0&quot;); &#125; if (machineId &gt; MAX_MACHINE_NUM || machineId &lt; 0) &#123; throw new IllegalArgumentException(&quot;machineId can&apos;t be greater than MAX_MACHINE_NUM or less than 0&quot;); &#125; this.datacenterId = datacenterId; this.machineId = machineId; &#125; /** * 产生下一个ID * @return */ public synchronized long nextId() &#123; long currStmp = getNewstmp(); if (currStmp &lt; lastStmp) &#123; throw new RuntimeException(&quot;Clock moved backwards. Refusing to generate id&quot;); &#125; if (currStmp == lastStmp) &#123; //相同毫秒内，序列号自增 sequence = (sequence + 1) &amp; MAX_SEQUENCE; //同一毫秒的序列数已经达到最大 if (sequence == 0L) &#123; currStmp = getNextMill(); &#125; &#125; else &#123; //不同毫秒内，序列号置为0 sequence = 0L; &#125; lastStmp = currStmp; return ( //时间戳部分 currStmp - START_STMP) &lt;&lt; TIMESTMP_LEFT //数据中心部分 | datacenterId &lt;&lt; DATACENTER_LEFT //机器标识部分 | machineId &lt;&lt; MACHINE_LEFT //序列号部分 | sequence; &#125; private long getNextMill() &#123; long mill = getNewstmp(); while (mill &lt;= lastStmp) &#123; mill = getNewstmp(); &#125; return mill; &#125; private long getNewstmp() &#123; return System.currentTimeMillis(); &#125; public static void main(String[] args) &#123; IdWorker snowFlake = new IdWorker(2, 3); long start = System.currentTimeMillis(); for (int i = 0; i &lt; 1000000; i++) &#123; System.out.println(snowFlake.nextId()); &#125; System.out.println(System.currentTimeMillis() - start); &#125;&#125; 最后大家可以看这也有更详细的解释","categories":[],"tags":[{"name":"订单号","slug":"订单号","permalink":"https://www.liangyouze.com/tags/订单号/"},{"name":"规则","slug":"规则","permalink":"https://www.liangyouze.com/tags/规则/"}]},{"title":"关于iframe跨域传输","slug":"关于iframe跨域传输","date":"2018-07-31T12:10:33.000Z","updated":"2018-10-08T05:52:24.000Z","comments":true,"path":"2018/07/31/关于iframe跨域传输/","link":"","permalink":"https://www.liangyouze.com/2018/07/31/关于iframe跨域传输/","excerpt":"至于我为什么想写这篇文章是因为最近在项目中使用到了iframe，是的。生无可恋的又写上了一点js，可能是因为前端的人对单点登录啥的或者是页面跳转以及要和后端的逻辑处理起来不是很熟练吧。各大网站，包括淘宝，京东，这些大网站有很多自己的产品，至于前期是怎么样的不是很清楚，网易云至少是用的iframe。参考了一些博客，至于使用不使用iframe，我觉得能解决问题就好，而且如果考虑的多的话就考虑以后扩展以及拆分啥的，毕竟前端又不像后端这样。","text":"至于我为什么想写这篇文章是因为最近在项目中使用到了iframe，是的。生无可恋的又写上了一点js，可能是因为前端的人对单点登录啥的或者是页面跳转以及要和后端的逻辑处理起来不是很熟练吧。各大网站，包括淘宝，京东，这些大网站有很多自己的产品，至于前期是怎么样的不是很清楚，网易云至少是用的iframe。参考了一些博客，至于使用不使用iframe，我觉得能解决问题就好，而且如果考虑的多的话就考虑以后扩展以及拆分啥的，毕竟前端又不像后端这样。 因为要解决跨域问题。有很多方案，比如说iframe，jsonp(不过只支持get，对于一些铭感信息就不行了) 原本需求是登录在一个站点，而注册是另外一个站点。因为要实时反馈到iframe子页面，子页面在进行相应。 而在Windows对象下有个postMessage方法，是解决跨越问题的假设有两个不同源的页面，iframe.html和index.html 其中前者是后者的子页面。123456789&lt;!-- index.html --&gt;&lt;body&gt; &lt;h1&gt;this is index&lt;/h1&gt; &lt;iframe src=\"./iframePage.html\" id='iframe'&gt;&lt;/iframe&gt;&lt;/body&gt; 1234567&lt;!-- iframePage --&gt;&lt;body&gt; &lt;h1&gt;this is iframePage&lt;/h1&gt;&lt;/body&gt; 现在这两个是无法通信的，因为是不同的站点，所以这个时候就要用到postMessage 123456789101112// idnex.html//获取iframe元素,当然也可以使用其他的js框架iFrame = document.getElementById('iframe')//iframe加载完毕后再发送消息，否则子页面接收不到messageiFrame.onload = function()&#123; //iframe加载完立即发送一条消息 iFrame.contentWindow.postMessage('MessageFromIndex1','*');&#125; 我们知道postMessage是挂载在window对象上的，所以等iframe加载完毕后，用iFrame.contentWindow获取到iframe的window对象，然后调用postMessage方法，相当于给子页面发送了一条消息。 postMessage方法第二个参数可以设置要发送到哪个url，如果当前子页面的url和设置的不一致，则会发送失败，因为没啥限制就设置为*，代表所有url都允许发送。 消息发送到iframePage.html，我们来接收message 123456789// iframePage.html//回调函数function receiveMessageFromIndex ( event ) &#123; console.log( 'receiveMessageFromIndex', event )&#125;//监听message事件window.addEventListener(\"message\", receiveMessageFromIndex, false); 然后设置好回调函数，就可以了，data中或许还有其他的数值，所以在接受的时候判断一下。","categories":[],"tags":[{"name":"跨域","slug":"跨域","permalink":"https://www.liangyouze.com/tags/跨域/"},{"name":"iframe","slug":"iframe","permalink":"https://www.liangyouze.com/tags/iframe/"}]},{"title":"初识Docker","slug":"关于docker","date":"2018-05-11T06:10:23.000Z","updated":"2018-11-18T12:42:13.000Z","comments":true,"path":"2018/05/11/关于docker/","link":"","permalink":"https://www.liangyouze.com/2018/05/11/关于docker/","excerpt":"关于dockerdocker是一款以容器虚拟化技术为基础的软件 那么什么是虚拟化技术 ？ 虚拟化技术是一种将计算机物理资源进行抽象、转换为虚拟的计算机资源提供给程序使用的技术。 因为要配置各种环境等，给开发造成了很多困扰。","text":"关于dockerdocker是一款以容器虚拟化技术为基础的软件 那么什么是虚拟化技术 ？ 虚拟化技术是一种将计算机物理资源进行抽象、转换为虚拟的计算机资源提供给程序使用的技术。 因为要配置各种环境等，给开发造成了很多困扰。 虚拟化还有一种作用，就是将虚拟化应用于资源管理。 假想一下，你要装mysql，redis等等，跑起一个服务端就比较费资源，虚拟化就可以很好地解决这件事情。就会有一种效果，那就是1+1&lt;2. 虚拟化技术通过资源隔离的方式，无形地也可以把这些程序隔离在不同的虚拟环境中，既然虚拟环境不同，自然运行在不同环境中的程序就不会互相干扰或争抢资源了。 docker的优势 基于容器技术的Docker拥有很高的跨平台性。Docker 的容器能够很轻松的运行在开发者本地的电脑，数据中心的物理机或虚拟机，云服务商提供的云服务器，甚至是混合环境中。 Docker 的轻量性和高可移植性能够很好的帮助我们完成应用的动态伸缩，我们可以通过一些手段近实时的对基于 Docker 运行的应用进行弹性伸缩，这能够大幅提高应用的健壮性。 不管是交付市场时间， 增加开发生产力，提高开发效率，节约基础设施成本，提升运维效率，以及加速问题解决时间。docker都有一个很好的作用。 关于docker的技术实现 Docker的实现，主要归结于三大技术，命令空间，控制组以及联合文件系统。大家可以更深入的去了解下。说到了Docker，就不得不先说说Docker的体系了。它有四个对象：镜像，容器，网络，数据卷。 镜像：大概可以理解为一个只读的文件包。其中包含了虚拟环境运行最原始文件系统的内容。镜像是对容器运行环境进行持久化存储的结果。 容器：容器就是用来隔离虚拟环境的基础设施，而在 Docker 里，它也被引申为隔离出来的虚拟环境。如果把镜像理解为编程中的类，那么容器就可以理解为类的实例。镜像内存放的是不可变化的东西，当以它们为基础的容器启动后，容器内也就成为了一个“活”的空间。 用更官方的定义来讲，Docker容器应该有三项内容组成。 一个Docker镜像 一个程序运行环境 一个指令集合 网络 对于大部分程序来说，它们的运行都不会是孤立的，而是要与外界或者更准确的说是与其他程序进行交互的，这里的交互绝大多数情况下指的就是数据信息的交换。网络通讯是目前最常用的一种程序间的数据交换方式了。 在 Docker 中，实现了强大的网络功能，我们不但能够十分轻松的对每个容器的网络进行配置，还能在容器间建立虚拟网络，将数个容器包裹其中，同时与其他网络环境隔离。 利用一些技术，Docker 能够在容器中营造独立的域名解析环境，这使得我们可以在不修改代码和配置的前提下直接迁移容器，Docker 会为我们完成新环境的网络适配。对于这个功能，我们甚至能够在不同的物理服务器间实现，让处在两台物理机上的两个 Docker 所提供的容器，加入到同一个虚拟网络中，形成完全屏蔽硬件的效果。 数据卷 在以往的虚拟机中，我们通常直接采用虚拟机的文件系统作为应用数据等文件的存储位置。然而这种方式其实并非完全安全的，当虚拟机或者容器出现问题导致文件系统无法使用时，虽然我们可以很快的通过镜像重置文件系统使得应用快速恢复运行，但是之前存放的数据也就消失了。 为了保证数据的独立性，我们通常会单独挂载一个文件系统来存放数据。这种操作在虚拟机中是繁琐的，因为我们不但要搞定挂载在不同宿主机中实现的方法，还要考虑挂载文件系统兼容性，虚拟操作系统配置等问题。值得庆幸的是，这些在 Docker 里都已经为我们轻松的实现了，我们只需要简单的一两个命令或参数，就能完成文件系统目录的挂载。","categories":[{"name":"docker","slug":"docker","permalink":"https://www.liangyouze.com/categories/docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://www.liangyouze.com/tags/Docker/"}]},{"title":"golang 中获取字符串个数","slug":"golang 中获取字符串个数","date":"2018-04-01T06:10:23.000Z","updated":"2019-08-11T03:21:34.614Z","comments":true,"path":"2018/04/01/golang 中获取字符串个数/","link":"","permalink":"https://www.liangyouze.com/2018/04/01/golang 中获取字符串个数/","excerpt":"golang 中获取字符串个数在 golang 中不能直接用 len 函数来统计字符串长度，查看了下源码发现字符串是以 UTF-8 为格式存储的，说明 len 函数是取得包含 byte 的个数","text":"golang 中获取字符串个数在 golang 中不能直接用 len 函数来统计字符串长度，查看了下源码发现字符串是以 UTF-8 为格式存储的，说明 len 函数是取得包含 byte 的个数 123// string is the set of all strings of 8-bit bytes, conventionally but not// necessarily representing UTF-8-encoded text. A string may be empty, but// not nil. Values of string type are immutable. 举个例子，”Hello, 世界“(因为，对比所以用了中文) 123s := \"Hello, 世界\"fmt.Println(len(s)) // 13fmt.Println([]byte(s)) // [72 101 108 108 111 44 32 228 184 150 231 149 140] 既然是以 byte 存储的，那自然就想到了取 byte 的长度 1234- bytes.Count() - strings.Count() - 将字符串转换为 []runee 后调用 len 函数- 使用 utf8.RuneCountInString() 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465package mainimport ( \"bytes\" \"fmt\" \"strings\" \"testing\" \"unicode/utf8\")/*在 golang 中不能直接用 len 函数来统计字符串长度，查看了下源码发现字符串是以 UTF-8 为格式存储的，说明 len 函数是取得包含 byte 的个数*/func main() &#123; s := \"hello, 世界\" fmt.Println(len(s)) // 13 fmt.Println([]byte(s)) // [72 101 108 108 111 44 32 228 184 150 231 149 140] fmt.Print(f1(s))&#125;func f1(s string) int &#123; return bytes.Count([]byte(s), nil) - 1&#125;func f2(s string) int &#123; return strings.Count(s, \"\") - 1&#125;func f3(s string) int &#123; return len([]rune(s))&#125;func f4(s string) int &#123; return utf8.RuneCountInString(s)&#125;var s = \"Hello, 世界\"func Benchmark1(b *testing.B) &#123; for i := 0; i &lt; b.N; i++ &#123; f1(s) &#125;&#125;func Benchmark2(b *testing.B) &#123; for i := 0; i &lt; b.N; i++ &#123; f2(s) &#125;&#125;func Benchmark3(b *testing.B) &#123; for i := 0; i &lt; b.N; i++ &#123; f3(s) &#125;&#125;func Benchmark4(b *testing.B) &#123; for i := 0; i &lt; b.N; i++ &#123; f4(s) &#125;&#125; 在 golang ldea配置中我没有看到 benchamark配置，总说包不对，在命令行中输入 1go test stringCount_test.go -bench \".*\" 得到以下结果 1234Benchmark1-12 100000000 17.7 ns/opBenchmark2-12 100000000 14.0 ns/opBenchmark3-12 100000000 14.5 ns/opBenchmark4-12 100000000 13.1 ns/op 最快的是utf8.RuneCountInString() 参考","categories":[{"name":"golang","slug":"golang","permalink":"https://www.liangyouze.com/categories/golang/"}],"tags":[{"name":"字符串","slug":"字符串","permalink":"https://www.liangyouze.com/tags/字符串/"}]},{"title":"count(*) 的实现方式","slug":"MySQL的count解读","date":"2018-03-11T06:10:23.000Z","updated":"2019-12-15T08:48:09.843Z","comments":true,"path":"2018/03/11/MySQL的count解读/","link":"","permalink":"https://www.liangyouze.com/2018/03/11/MySQL的count解读/","excerpt":"InnoDB引擎在执行count(*)的时候，需要把数据一行一行地从引擎里面读出来，然后累积计数 MyISAM 引擎把一个表的总行数存在了磁盘上，因此执行 count(*) 的时候会直接返回这个数，效率很高；","text":"InnoDB引擎在执行count(*)的时候，需要把数据一行一行地从引擎里面读出来，然后累积计数 MyISAM 引擎把一个表的总行数存在了磁盘上，因此执行 count(*) 的时候会直接返回这个数，效率很高； 那为什么 InnoDB 不跟 MyISAM 一样，也把数字存起来呢？这是因为即使是在同一个时刻的多个查询，由于多版本并发控制（MVCC）的原因，InnoDB 表“应该返回多少行”也是不确定的。这里，用一个算 count(*) 的例子来为你解释一下。 假设表 t 中现在有 10000 条记录，我们设计了三个用户并行的会话。 会话 A 先启动事务并查询一次表的总行数； 会话 B 启动事务，插入一行后记录后，查询表的总行数； 会话 C 先启动一个单独的语句，插入一行记录后，查询表的总行数。 我们假设从上到下是按照时间顺序执行的，同一行语句是在同一时刻执行的。 会话A 会话B 会话C begin select count (*) from t insert into t (写入一行数据) begin insert into t(写入一行数据) select count(*) from t(返回10000) select count(*) from t(返回10002) select count(*) from t(返回10001) 在最后一个时刻，三个会话A，B，C会同时查询表t的总行数，但拿到的结果却不同 这和 InnoDB 的事务设计有关系，可重复读是它默认的隔离级别，在代码上就是通过多版本并发控制，也就是 MVCC 来实现的。每一行记录都要判断自己是否对这个会话可见，因此对于 count(*) 请求来说，InnoDB 只好把数据一行一行地读出依次判断，可见的行才能够用于计算“基于这个查询”的表的总行数。 InnoDB 是索引组织表，主键索引树的叶子节点是数据，而普通索引树的叶子节点是主键值。所以，普通索引树比主键索引树小很多。对于 count(*) 这样的操作，遍历哪个索引树得到的结果逻辑上都是一样的。因此，MySQL 优化器会找到最小的那棵树来遍历。在保证逻辑正确的前提下，尽量减少扫描的数据量，是数据库系统设计的通用法则之一。 如果你用过 show table status 命令的话，就会发现这个命令的输出结果里面也有一个 TABLE_ROWS 用于显示这个表当前有多少行，这个命令执行挺快的，那这个 TABLE_ROWS 能代替 count(*) 吗？ 索引统计的值是通过采样来估算的。实际上，TABLE_ROWS 就是从这个采样估算得来的，因此它也很不准。有多不准呢，官方文档说误差可能达到 40% 到 50%。所以，show table status 命令显示的行数也不能直接使用。 总结一下 MyISAM 表虽然 count(*) 很快，但是不支持事务； show table status 命令虽然返回很快，但是不准确； InnoDB 表直接 count(*) 会遍历全表，虽然结果准确，但会导致性能问题。 对于 count(主键 id) 来说，InnoDB 引擎会遍历整张表，把每一行的 id 值都取出来，返回给 server 层。server 层拿到 id 后，判断是不可能为空的，就按行累加。 对于 count(1) 来说，InnoDB 引擎遍历整张表，但不取值。server 层对于返回的每一行，放一个数字“1”进去，判断是不可能为空的，按行累加。 对于 count(字段) 来说： 如果这个“字段”是定义为 not null 的话，一行行地从记录里面读出这个字段，判断不能为 null，按行累加； 如果这个“字段”定义允许为 null，那么执行的时候，判断到有可能是 null，还要把值取出来再判断一下，不是 null 才累加。 按照效率排序的话，count(字段)&lt;count(主键id)&lt;count(1)≈&lt;count(*) 尽量使用count(*) 参考极客时间MySQL实战45讲","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://www.liangyouze.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://www.liangyouze.com/tags/MySQL/"}]},{"title":"String、StringBuffer、StringBuilder三者之间的区别","slug":"String、StringBuffer、StringBuilder三者之间的区别","date":"2017-11-23T14:38:31.000Z","updated":"2018-10-08T05:51:52.000Z","comments":true,"path":"2017/11/23/String、StringBuffer、StringBuilder三者之间的区别/","link":"","permalink":"https://www.liangyouze.com/2017/11/23/String、StringBuffer、StringBuilder三者之间的区别/","excerpt":"吧啦吧啦，今天在公司做算法题的时候，不仅就想写下了 String是不可变类，所以任何对String的操作都将引发新的String对象的生成。但是StringBuffer是可变类，任何对StringBuffer所指代的字符串改变都不会产生新的对象。 新引入的StingBuilder类不是线程安全，但其在单线程中的性能比StringBuffer高。","text":"吧啦吧啦，今天在公司做算法题的时候，不仅就想写下了 String是不可变类，所以任何对String的操作都将引发新的String对象的生成。但是StringBuffer是可变类，任何对StringBuffer所指代的字符串改变都不会产生新的对象。 新引入的StingBuilder类不是线程安全，但其在单线程中的性能比StringBuffer高。 下面是一点小例子 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102import java.util.ArrayList;import java.util.Iterator;import java.util.List;/** * 从JDK1.5中,有了StringBuilder。 */public class DifferenceStringBufferAndStringBuilder &#123; private static final String base = &quot;String&quot;; private static final int count = 3000000; public static void main(String[] args) &#123; stringTest(); stringBufferTest(); stringBuilderTest(); addToStringBuilder(); addToStringBuffer(); &#125; /** * string执行性能测试 */ public static void stringTest() &#123; long begin, end; begin = System.currentTimeMillis(); String test = new String(base); // 在这里为什么要缩150，因为其实时间是很长的 for (int i = 0; i &lt; count / 150; i++) &#123; test = test + &quot;add&quot;; &#125; end = System.currentTimeMillis(); System.out.println((end - begin) + &quot;millis has elapsed when used String&quot;); &#125; /** * stringBuffer */ public static void stringBufferTest() &#123; long begin, end; begin = System.currentTimeMillis(); StringBuffer stringBuffer = new StringBuffer(base); for (int i = 0; i &lt; count; i++) &#123; stringBuffer.append(&quot;add&quot;); &#125; end = System.currentTimeMillis(); System.out.println((end - begin) + &quot;millis has elapsed when used StringBuffer&quot;); &#125; /** * stingBuilder */ public static void stringBuilderTest() &#123; long begin, end; begin = System.currentTimeMillis(); StringBuilder stringBuilder = new StringBuilder(base); for (int i = 0; i &lt; count; i++) &#123; stringBuilder.append(&quot;add&quot;); &#125; end = System.currentTimeMillis(); System.out.println((end - begin) + &quot;mills has elapsed when used StringBuilder&quot;); &#125; /** *转换为StringBuilder */ public static String appendItemsToStringBuilder(List list)&#123; StringBuilder stringBuilder = new StringBuilder(); for (Iterator i = list.iterator();i.hasNext();)&#123; stringBuilder.append(i.next()).append(&quot;&quot;); &#125; return stringBuilder.toString(); &#125; public static void addToStringBuilder()&#123; List list = new ArrayList(); list.add(&quot;l&quot;); list.add(&quot;y&quot;); list.add(&quot;z&quot;); System.out.println(DifferenceStringBufferAndStringBuilder.appendItemsToStringBuilder(list)); &#125; public static String appendItemsToStringBuffer(List list)&#123; StringBuffer stringBuffer = new StringBuffer(); for (Iterator i = list.iterator();i.hasNext();)&#123; stringBuffer.append(i.next()).append(&quot;&quot;); &#125; return stringBuffer.toString(); &#125; public static void addToStringBuffer()&#123; List list = new ArrayList(); list.add(&quot;l&quot;); list.add(&quot;y&quot;); list.add(&quot;z&quot;); System.out.println(DifferenceStringBufferAndStringBuilder.appendItemsToStringBuffer(list)); &#125;&#125; 最后输出的是 123451127millis has elapsed when used String86millis has elapsed when used StringBuffer35mills has elapsed when used StringBuilderlyzlyz 所以根据结果来看，采用String对象时，哪怕是次数是其他对象的1/150,执行时间上也比其他对象高很多，而采用StringBuffer对象和采用StringBuilder对象也有明显的差距。所以如果是在单线程下运行，就不必考虑到线程同步的问题，优先采用StringBuilder类，当然，如果是要保证线程安全的话，就要考虑到StringBuffer了。 除了对多线程的支持不一样的话，其实这两个类没啥区别的，上面不就很好的说明了嘛。","categories":[],"tags":[{"name":"字符串","slug":"字符串","permalink":"https://www.liangyouze.com/tags/字符串/"}]}]}